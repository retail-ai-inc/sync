package backup

import (
	"bufio"
	"context"
	"encoding/json"
	"fmt"
	"os"
	"os/exec"
	"path/filepath"
	"runtime"
	"strings"
	"time"

	"github.com/sirupsen/logrus"
)

// ExecuteExternalMongoBackup ä½¿ç”¨å¤–éƒ¨å‘½ä»¤æ‰§è¡ŒMongoDBå¤‡ä»½
// é¿å…Goå†…å­˜ç®¡ç†é—®é¢˜ï¼Œç›´æ¥è°ƒç”¨ç³»ç»Ÿå‘½ä»¤
func (e *BackupExecutor) ExecuteExternalMongoBackup(ctx context.Context, config ExecutorBackupConfig, tempDir string, task BackupTask, collection string) error {
	logrus.Infof("[BackupExecutor] ğŸš€ Using EXTERNAL COMMAND mode for collection: %s", collection)

	// è®°å½•Goè¿›ç¨‹å†…å­˜ (åº”è¯¥ä¿æŒç¨³å®š)
	e.logMemoryUsage("EXTERNAL_MODE_START")

	// æ„å»ºè¿æ¥å­—ç¬¦ä¸²
	connStr := buildMongoDBConnectionString(config.Database.URL, config.Database.Username, config.Database.Password)

	// æ–‡ä»¶è·¯å¾„
	dateStr := time.Now().AddDate(0, 0, -1).Format("2006-01-02")
	outputPath := fmt.Sprintf("%s/%s_%s.json", tempDir, collection, dateStr)
	zipPath := fmt.Sprintf("%s/%s_%s.zip", tempDir, collection, dateStr)

	// Step 1: å¤–éƒ¨mongoexportå‘½ä»¤
	logrus.Infof("[BackupExecutor] ğŸ“¤ Step 1: External mongoexport")
	if err := e.executeExternalMongoExport(ctx, connStr, config.Database.Database, collection, outputPath); err != nil {
		return fmt.Errorf("external mongoexport failed: %w", err)
	}

	e.logMemoryUsage("AFTER_MONGOEXPORT")

	// Step 2: å¤–éƒ¨zipå‘½ä»¤
	logrus.Infof("[BackupExecutor] ğŸ—œï¸ Step 2: External zip compression")
	if err := e.executeExternalZip(ctx, tempDir, outputPath, zipPath); err != nil {
		return fmt.Errorf("external zip failed: %w", err)
	}

	e.logMemoryUsage("AFTER_ZIP")

	// Step 3: å¤–éƒ¨gsutilä¸Šä¼  (å¦‚æœé…ç½®äº†GCS)
	if config.Destination.GCSPath != "" {
		logrus.Infof("[BackupExecutor] â˜ï¸ Step 3: External GCS upload")
		gcsPath := fmt.Sprintf("%s/%s_%s.zip", config.Destination.GCSPath, collection, dateStr)
		if err := e.executeExternalGCSUpload(ctx, zipPath, gcsPath); err != nil {
			return fmt.Errorf("external GCS upload failed: %w", err)
		}
	}

	e.logMemoryUsage("EXTERNAL_MODE_COMPLETE")

	// æš‚æ—¶ä¸åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼Œä¿ç•™ç”¨äºè°ƒè¯•åˆ†æ
	logrus.Infof("[BackupExecutor] ğŸ” Keeping JSON file for debugging: %s", outputPath)
	logrus.Infof("[BackupExecutor] ğŸ” Keeping ZIP file for debugging: %s", zipPath)
	// ä¿ç•™æ‰€æœ‰æ–‡ä»¶ä¾›è°ƒè¯•åˆ†æ

	logrus.Infof("[BackupExecutor] âœ… External backup completed for collection: %s", collection)
	return nil
}

// executeExternalMongoExport æ‰§è¡Œå¤–éƒ¨mongoexportå‘½ä»¤
func (e *BackupExecutor) executeExternalMongoExport(ctx context.Context, connStr, database, collection, outputPath string) error {
	cmd := exec.CommandContext(ctx, "mongoexport",
		"--uri", connStr,
		"--db", database,
		"--collection", collection,
		"--out", outputPath,
		"--quiet")

	logrus.Infof("[BackupExecutor] Executing: mongoexport --db %s --collection %s --out %s", database, collection, outputPath)

	output, err := cmd.CombinedOutput()
	if err != nil {
		return fmt.Errorf("mongoexport failed: %w, output: %s", err, string(output))
	}

	// æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
	if _, err := os.Stat(outputPath); err != nil {
		return fmt.Errorf("mongoexport output file not created: %w", err)
	}

	// è®°å½•æ–‡ä»¶å¤§å°
	if stat, err := os.Stat(outputPath); err == nil {
		logrus.Infof("[BackupExecutor] âœ… Mongoexport completed: %.2f MB", float64(stat.Size())/1024/1024)
	}

	return nil
}

// executeExternalZip æ‰§è¡Œå¤–éƒ¨zipå‘½ä»¤
func (e *BackupExecutor) executeExternalZip(ctx context.Context, workDir, inputFile, outputFile string) error {
	// ä½¿ç”¨ç³»ç»Ÿzipå‘½ä»¤
	cmd := exec.CommandContext(ctx, "zip", "-j", outputFile, inputFile)
	cmd.Dir = workDir

	logrus.Infof("[BackupExecutor] Executing: zip -j %s %s", outputFile, inputFile)

	output, err := cmd.CombinedOutput()
	if err != nil {
		return fmt.Errorf("zip failed: %w, output: %s", err, string(output))
	}

	// æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
	if _, err := os.Stat(outputFile); err != nil {
		return fmt.Errorf("zip output file not created: %w", err)
	}

	// è®°å½•å‹ç¼©æ•ˆæœ
	if stat, err := os.Stat(outputFile); err == nil {
		logrus.Infof("[BackupExecutor] âœ… Zip completed: %.2f MB", float64(stat.Size())/1024/1024)
	}

	return nil
}

// executeExternalGCSUpload æ‰§è¡Œå¤–éƒ¨gsutilä¸Šä¼ 
func (e *BackupExecutor) executeExternalGCSUpload(ctx context.Context, localFile, gcsPath string) error {
	cmd := exec.CommandContext(ctx, "gsutil", "cp", localFile, gcsPath)

	logrus.Infof("[BackupExecutor] Executing: gsutil cp %s %s", localFile, gcsPath)

	output, err := cmd.CombinedOutput()
	if err != nil {
		return fmt.Errorf("gsutil upload failed: %w, output: %s", err, string(output))
	}

	logrus.Infof("[BackupExecutor] âœ… GCS upload completed: %s", gcsPath)
	return nil
}

// copyExistingFile å¤åˆ¶ç°æœ‰æ•°æ®æ–‡ä»¶ç”¨äºæµ‹è¯•
func (e *BackupExecutor) copyExistingFile(src, dst string) error {
	// è®°å½•å¼€å§‹æ—¶å†…å­˜
	var memStats runtime.MemStats
	runtime.ReadMemStats(&memStats)
	logrus.Infof("[BackupExecutor] ğŸ”„ Memory BEFORE file copy: Alloc=%.2fMB, Sys=%.2fMB",
		float64(memStats.Alloc)/1024/1024,
		float64(memStats.Sys)/1024/1024)

	// ä½¿ç”¨ç³»ç»Ÿcpå‘½ä»¤å¤åˆ¶æ–‡ä»¶ä»¥é¿å…Goå†…å­˜ä½¿ç”¨
	cmd := exec.CommandContext(context.Background(), "cp", src, dst)

	logrus.Infof("[BackupExecutor] ğŸ”„ Executing: cp %s %s", src, dst)

	output, err := cmd.CombinedOutput()
	if err != nil {
		return fmt.Errorf("cp command failed: %w, output: %s", err, string(output))
	}

	// éªŒè¯å¤åˆ¶ç»“æœ
	if _, err := os.Stat(dst); err != nil {
		return fmt.Errorf("copied file not found: %w", err)
	}

	// è®°å½•å¤åˆ¶åå†…å­˜
	runtime.ReadMemStats(&memStats)
	logrus.Infof("[BackupExecutor] ğŸ”„ Memory AFTER file copy: Alloc=%.2fMB, Sys=%.2fMB",
		float64(memStats.Alloc)/1024/1024,
		float64(memStats.Sys)/1024/1024)

	return nil
}

// logMemoryUsage è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ
func (e *BackupExecutor) logMemoryUsage(phase string) {
	var m runtime.MemStats
	runtime.ReadMemStats(&m)

	logrus.Infof("[BackupExecutor] ğŸ“Š Go Memory [%s]: Alloc=%.2fMB, Sys=%.2fMB, NumGoroutines=%d",
		phase,
		float64(m.Alloc)/1024/1024,
		float64(m.Sys)/1024/1024,
		runtime.NumGoroutine())
}

// executeExternalMongoExportSimple å®Œæ•´çš„å¤–éƒ¨å‘½ä»¤å¤‡ä»½ï¼šmongoexport -> zip -> GCS upload
func (e *BackupExecutor) executeExternalMongoExportSimple(ctx context.Context, connStr, database, collection, tempDir string, config ExecutorBackupConfig) error {
	logrus.Infof("[BackupExecutor] ğŸš€ Starting COMPLETE external command backup for collection: %s", collection)

	// è®°å½•Goè¿›ç¨‹å†…å­˜ (åº”è¯¥ä¿æŒç¨³å®š)
	e.logMemoryUsage("EXTERNAL_FULL_START")

	// æ–‡ä»¶è·¯å¾„
	dateStr := time.Now().AddDate(0, 0, -1).Format("2006-01-02")
	outputPath := fmt.Sprintf("%s/%s_%s.json", tempDir, collection, dateStr)
	zipPath := fmt.Sprintf("%s/%s_%s.zip", tempDir, collection, dateStr)

	// Step 1: ä½¿ç”¨ç°æœ‰æ•°æ®æ–‡ä»¶ (è·³è¿‡mongoexportä»¥èŠ‚çœæ—¶é—´)
	logrus.Infof("[BackupExecutor] ğŸ“¤ Step 1: Using existing data file (SKIP mongoexport)")

	// æ£€æŸ¥ç°æœ‰æ•°æ®æ–‡ä»¶è·¯å¾„ (æ”¯æŒæœ¬åœ°æµ‹è¯•)
	existingFiles := []string{
		"/mnt/state/RetailerRecommendationAnalytics_202508_2025-08-26.json",     // æœåŠ¡å™¨è·¯å¾„
		"/tmp/mnt/state/RetailerRecommendationAnalytics_202508_2025-08-26.json", // æœ¬åœ°æµ‹è¯•è·¯å¾„
	}

	var existingFile string
	for _, file := range existingFiles {
		if _, err := os.Stat(file); err == nil {
			existingFile = file
			break
		}
	}

	if existingFile != "" {
		logrus.Infof("[BackupExecutor] âœ… Found existing file: %s", existingFile)

		// å¤åˆ¶ç°æœ‰æ–‡ä»¶åˆ°è¾“å‡ºè·¯å¾„
		if err := e.copyExistingFile(existingFile, outputPath); err != nil {
			return fmt.Errorf("failed to copy existing file: %w", err)
		}
		logrus.Infof("[BackupExecutor] âœ… Copied existing file to: %s", outputPath)

		// è®°å½•æ–‡ä»¶å¤§å°
		if stat, err := os.Stat(outputPath); err == nil {
			logrus.Infof("[BackupExecutor] ğŸ“Š Data file size: %.2f MB", float64(stat.Size())/1024/1024)
		}
	} else {
		// å¦‚æœæ²¡æœ‰ç°æœ‰æ–‡ä»¶ï¼Œå›é€€åˆ°mongoexport with query conditions
		logrus.Infof("[BackupExecutor] âš ï¸ Existing file not found, falling back to mongoexport with query conditions")
		if err := e.executeExternalMongoExportWithOptions(ctx, connStr, database, collection, outputPath, config); err != nil {
			return fmt.Errorf("external mongoexport failed: %w", err)
		}
	}

	e.logMemoryUsage("AFTER_EXTERNAL_EXPORT")

	// Step 2: å¤–éƒ¨zipå‘½ä»¤
	logrus.Infof("[BackupExecutor] ğŸ—œï¸ Step 2: External zip compression")
	if err := e.executeExternalZip(ctx, tempDir, outputPath, zipPath); err != nil {
		return fmt.Errorf("external zip failed: %w", err)
	}

	e.logMemoryUsage("AFTER_EXTERNAL_ZIP")

	// Step 3: å¤–éƒ¨GCSä¸Šä¼  (éœ€è¦é…ç½®GCSè·¯å¾„)
	// TODO: ä»é…ç½®ä¸­è·å–GCSè·¯å¾„
	gcsPath := fmt.Sprintf("gs://logs-router-bucketbk/external/%s_%s.zip", collection, dateStr)
	logrus.Infof("[BackupExecutor] â˜ï¸ Step 3: External GCS upload")
	if err := e.executeExternalGCSUpload(ctx, zipPath, gcsPath); err != nil {
		return fmt.Errorf("external GCS upload failed: %w", err)
	}

	e.logMemoryUsage("EXTERNAL_FULL_COMPLETE")

	// æš‚æ—¶ä¸åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼Œä¿ç•™ç”¨äºè°ƒè¯•åˆ†æ
	logrus.Infof("[BackupExecutor] ğŸ” Keeping JSON file for debugging: %s", outputPath)
	logrus.Infof("[BackupExecutor] ğŸ” Keeping ZIP file for debugging: %s", zipPath)
	// ä¿ç•™æ‰€æœ‰æ–‡ä»¶ä¾›è°ƒè¯•åˆ†æ

	logrus.Infof("[BackupExecutor] âœ… COMPLETE external backup workflow completed for collection: %s", collection)

	return nil
}

// UseExternalCommands æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨å¤–éƒ¨å‘½ä»¤æ¨¡å¼
func (e *BackupExecutor) UseExternalCommands() bool {
	// å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶
	if os.Getenv("USE_EXTERNAL_BACKUP") == "true" {
		return true
	}

	// ä¹Ÿå¯ä»¥æ£€æŸ¥å¯ç”¨å†…å­˜ï¼Œå¦‚æœå†…å­˜ä¸è¶³è‡ªåŠ¨åˆ‡æ¢åˆ°å¤–éƒ¨å‘½ä»¤æ¨¡å¼
	var m runtime.MemStats
	runtime.ReadMemStats(&m)
	currentMB := float64(m.Alloc) / 1024 / 1024

	if currentMB > 2000 { // å¦‚æœGoè¿›ç¨‹å·²ç»ä½¿ç”¨è¶…è¿‡2GBï¼Œåˆ‡æ¢åˆ°å¤–éƒ¨æ¨¡å¼
		logrus.Warnf("[BackupExecutor] High memory usage detected (%.2fMB), switching to external command mode", currentMB)
		return true
	}

	return false
}

// exportMongoDBMergedTables ä½¿ç”¨å¤–éƒ¨å‘½ä»¤è¿›è¡Œå¤šè¡¨åˆå¹¶å¤‡ä»½
// å¤„ç†è·¨æœˆæ•°æ®å¯¼å‡ºåœºæ™¯ï¼Œæ”¯æŒå¤šä¸ªé›†åˆçš„åˆå¹¶ï¼Œå‚è€ƒexecuteExternalMongoExportSimpleçš„å®ç°æ¨¡å¼
func (e *BackupExecutor) exportMongoDBMergedTables(ctx context.Context, connStr, database string, tables []string, tempDir string, config ExecutorBackupConfig) error {
	logrus.Infof("[BackupExecutor] ğŸš€ Starting multi-table merge backup for %d tables: %v", len(tables), tables)

	// è®°å½•Goè¿›ç¨‹å†…å­˜ (åº”è¯¥ä¿æŒç¨³å®š)
	e.logMemoryUsage("MERGED_TABLES_START")

	// æå–åŸºç¡€åç§°ï¼ˆå»æ‰æ—¥æœŸåç¼€ï¼‰
	baseCollectionName := e.extractTablePrefix(tables[0])

	// ä½¿ç”¨ processFileNamePattern ç”Ÿæˆæ­£ç¡®çš„æ–‡ä»¶å
	fileName := processFileNamePattern(config.Destination.FileNamePattern, baseCollectionName)

	// ç¡®ä¿æ‰©å±•åä¸º .json
	if !strings.HasSuffix(fileName, ".json") {
		fileName = strings.TrimSuffix(fileName, filepath.Ext(fileName)) + ".json"
	}

	mergedJsonPath := filepath.Join(tempDir, fileName)

	// ç”Ÿæˆå¯¹åº”çš„zipæ–‡ä»¶å
	zipFileName := strings.TrimSuffix(fileName, ".json") + ".zip"
	zipPath := filepath.Join(tempDir, zipFileName)

	// Step 1: åˆ†åˆ«å¯¼å‡ºæ¯ä¸ªè¡¨å¹¶åˆå¹¶
	logrus.Infof("[BackupExecutor] ğŸ“¤ Step 1: Exporting and merging %d tables", len(tables))

	// åˆ›å»ºåˆå¹¶æ–‡ä»¶
	mergedFile, err := os.Create(mergedJsonPath)
	if err != nil {
		return fmt.Errorf("failed to create merged file: %w", err)
	}
	defer mergedFile.Close()

	// å†™å…¥JSONæ•°ç»„å¼€å§‹ç¬¦å·
	if _, err := mergedFile.WriteString("[\n"); err != nil {
		return fmt.Errorf("failed to write array start: %w", err)
	}

	for i, table := range tables {
		logrus.Infof("[BackupExecutor] ğŸ“„ Exporting table %d/%d: %s", i+1, len(tables), table)

		// ä¸ºæ¯ä¸ªè¡¨åˆ›å»ºä¸´æ—¶æ–‡ä»¶
		dateStr := time.Now().AddDate(0, 0, -1).Format("2006-01-02")
		tempTablePath := fmt.Sprintf("%s/%s_%s_temp.json", tempDir, table, dateStr)

		// ä½¿ç”¨mongoexportå¯¼å‡ºå•ä¸ªè¡¨ï¼Œåº”ç”¨æŸ¥è¯¢æ¡ä»¶å’Œå­—æ®µé€‰æ‹©
		if err := e.executeExternalMongoExportWithOptions(ctx, connStr, database, table, tempTablePath, config); err != nil {
			// å¦‚æœæ˜¯å› ä¸ºæ²¡æœ‰æŸ¥è¯¢æ¡ä»¶è€Œè·³è¿‡ï¼Œåˆ™ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªè¡¨
			if strings.Contains(err.Error(), "no query conditions specified") {
				logrus.Infof("[BackupExecutor] â­ï¸  Skipping table %s (no query conditions)", table)
				continue
			}
			return fmt.Errorf("failed to export table %s: %w", table, err)
		}

		// è¯»å–ä¸´æ—¶æ–‡ä»¶å¹¶åˆå¹¶åˆ°ä¸»æ–‡ä»¶
		tempFile, err := os.Open(tempTablePath)
		if err != nil {
			return fmt.Errorf("failed to open temp file for table %s: %w", table, err)
		}

		// è¯»å–JSONLæ ¼å¼æ–‡ä»¶å†…å®¹ï¼ˆmongoexporté»˜è®¤è¾“å‡ºæ ¼å¼ï¼‰
		content, err := os.ReadFile(tempTablePath)
		if err != nil {
			tempFile.Close()
			return fmt.Errorf("failed to read temp file for table %s: %w", table, err)
		}

		// mongoexportè¾“å‡ºçš„æ˜¯JSONLæ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰ï¼Œä¸æ˜¯JSONæ•°ç»„
		// éœ€è¦å°†æ¯è¡Œè½¬æ¢ä¸ºæ•°ç»„å…ƒç´ 
		contentStr := strings.TrimSpace(string(content))

		if len(contentStr) > 0 {
			// å°†JSONLæ ¼å¼è½¬æ¢ä¸ºJSONæ•°ç»„å…ƒç´ 
			lines := strings.Split(contentStr, "\n")
			var validLines []string

			for _, line := range lines {
				line = strings.TrimSpace(line)
				if line != "" && strings.HasPrefix(line, "{") {
					validLines = append(validLines, line)
				}
			}

			// å¦‚æœæœ‰æœ‰æ•ˆæ•°æ®è¡Œ
			if len(validLines) > 0 {
				// å¦‚æœä¸æ˜¯ç¬¬ä¸€ä¸ªè¡¨ï¼Œæ·»åŠ é€—å·åˆ†éš”ç¬¦
				if i > 0 {
					if _, err := mergedFile.WriteString(",\n"); err != nil {
						tempFile.Close()
						return fmt.Errorf("failed to write separator: %w", err)
					}
				}

				// å†™å…¥æ¯ä¸ªJSONå¯¹è±¡ï¼Œç”¨é€—å·åˆ†éš”
				for j, line := range validLines {
					if j > 0 {
						if _, err := mergedFile.WriteString(",\n"); err != nil {
							tempFile.Close()
							return fmt.Errorf("failed to write line separator: %w", err)
						}
					}
					if _, err := mergedFile.WriteString(line); err != nil {
						tempFile.Close()
						return fmt.Errorf("failed to write line data for %s: %w", table, err)
					}
				}
			}
		}

		tempFile.Close()
		// æš‚æ—¶ä¸åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼Œä¿ç•™ç”¨äºè°ƒè¯•
		logrus.Infof("[BackupExecutor] ğŸ” Keeping temp file for debugging: %s", tempTablePath)

		logrus.Infof("[BackupExecutor] âœ… Table %s merged successfully", table)
	}

	// å†™å…¥JSONæ•°ç»„ç»“æŸç¬¦å·
	if _, err := mergedFile.WriteString("\n]"); err != nil {
		return fmt.Errorf("failed to write array end: %w", err)
	}
	mergedFile.Close()

	// æ£€æŸ¥åˆå¹¶æ–‡ä»¶
	if stat, err := os.Stat(mergedJsonPath); err == nil {
		logrus.Infof("[BackupExecutor] âœ… Merge completed: %.2f MB", float64(stat.Size())/1024/1024)

		// é¢å¤–è°ƒè¯•ï¼šæ˜¾ç¤ºæ–‡ä»¶å†…å®¹çš„å‰å‡ è¡Œå’Œåå‡ è¡Œ
		if content, readErr := os.ReadFile(mergedJsonPath); readErr == nil {
			contentStr := string(content)
			lines := strings.Split(contentStr, "\n")
			logrus.Infof("[BackupExecutor] ğŸ” Merged file has %d lines", len(lines))

			// æ˜¾ç¤ºå‰3è¡Œ
			for i := 0; i < 3 && i < len(lines); i++ {
				logrus.Infof("[BackupExecutor] ğŸ” Line %d: %s", i+1, lines[i])
			}

			// æ˜¾ç¤ºå3è¡Œ
			if len(lines) > 3 {
				logrus.Infof("[BackupExecutor] ğŸ” ...")
				for i := len(lines) - 3; i < len(lines); i++ {
					if i >= 0 {
						logrus.Infof("[BackupExecutor] ğŸ” Line %d: %s", i+1, lines[i])
					}
				}
			}
		}
	} else {
		logrus.Errorf("[BackupExecutor] âŒ Failed to stat merged file: %v", err)
	}

	e.logMemoryUsage("AFTER_MERGE")

	// Step 2: å¤–éƒ¨zipå‘½ä»¤
	logrus.Infof("[BackupExecutor] ğŸ—œï¸ Step 2: External zip compression")
	if err := e.executeExternalZip(ctx, tempDir, mergedJsonPath, zipPath); err != nil {
		return fmt.Errorf("external zip failed: %w", err)
	}

	e.logMemoryUsage("AFTER_EXTERNAL_ZIP")

	// Step 3: å¤–éƒ¨GCSä¸Šä¼ 
	gcsPath := fmt.Sprintf("%s/%s", config.Destination.GCSPath, zipFileName)
	if !strings.HasPrefix(gcsPath, "gs://") {
		gcsPath = fmt.Sprintf("gs://logs-router-bucketbk/external/%s", zipFileName)
	}
	logrus.Infof("[BackupExecutor] â˜ï¸ Step 3: External GCS upload")
	if err := e.executeExternalGCSUpload(ctx, zipPath, gcsPath); err != nil {
		return fmt.Errorf("external GCS upload failed: %w", err)
	}

	e.logMemoryUsage("MERGED_TABLES_COMPLETE")

	// æš‚æ—¶ä¸åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼Œä¿ç•™ç”¨äºè°ƒè¯•
	logrus.Infof("[BackupExecutor] ğŸ” Keeping merged file for debugging: %s", mergedJsonPath)
	logrus.Infof("[BackupExecutor] ğŸ” Keeping zip file for debugging: %s", zipPath)
	// ä¿ç•™æ‰€æœ‰æ–‡ä»¶ä¾›åç»­è°ƒè¯•åˆ†æ

	logrus.Infof("[BackupExecutor] âœ… Multi-table merge backup completed successfully for %d tables", len(tables))
	return nil
}

// executeExternalMongoExportWithOptions æ‰§è¡Œå¤–éƒ¨mongoexportå‘½ä»¤ï¼Œæ”¯æŒæŸ¥è¯¢æ¡ä»¶å’Œå­—æ®µé€‰æ‹©
func (e *BackupExecutor) executeExternalMongoExportWithOptions(ctx context.Context, connStr, database, collection, outputPath string, config ExecutorBackupConfig) error {
	args := []string{
		"--uri", connStr,
		"--db", database,
		"--collection", collection,
		"--out", outputPath,
		"--quiet",
	}

	// æ·»åŠ æŸ¥è¯¢æ¡ä»¶
	if queryConditions, exists := config.Query[collection]; exists && len(queryConditions) > 0 {
		// æ¸…ç†æŸ¥è¯¢æ¡ä»¶ä¸­çš„å¤šä½™å¼•å·
		cleanedQuery := cleanQueryStringValues(queryConditions)

		// è½¬æ¢åŠ¨æ€æ—¶é—´æŸ¥è¯¢ä¸ºå…·ä½“çš„MongoDBæŸ¥è¯¢
		finalQuery := e.convertTimeRangeQuery(cleanedQuery)

		queryJSON, err := json.Marshal(finalQuery)
		if err != nil {
			logrus.Warnf("[BackupExecutor] Failed to marshal query for collection %s: %v", collection, err)
		} else {
			args = append(args, "--query", string(queryJSON))
			logrus.Infof("[BackupExecutor] Applied query for collection %s: %s", collection, string(queryJSON))
		}
	} else {
		// å¦‚æœæ²¡æœ‰æŸ¥è¯¢æ¡ä»¶ï¼Œè·³è¿‡è¯¥è¡¨çš„å¯¼å‡º
		logrus.Warnf("[BackupExecutor] âš ï¸  No query conditions found for collection %s, skipping export", collection)
		return fmt.Errorf("no query conditions specified for collection %s", collection)
	}

	// æ·»åŠ å­—æ®µé€‰æ‹©
	if fields, exists := config.Database.Fields[collection]; exists && len(fields) > 0 && fields[0] != "all" {
		fieldsStr := strings.Join(fields, ",")
		args = append(args, "--fields", fieldsStr)
		logrus.Infof("[BackupExecutor] Applied field selection for collection %s: %s", collection, fieldsStr)
	}

	cmd := exec.CommandContext(ctx, "mongoexport", args...)

	// æ˜¾ç¤ºå®Œæ•´çš„å‘½ä»¤è¡Œå‚æ•°ï¼ŒåŒ…æ‹¬queryå‚æ•°
	logrus.Infof("[BackupExecutor] Executing: %s", strings.Join(append([]string{"mongoexport"}, args...), " "))

	output, err := cmd.CombinedOutput()
	if err != nil {
		return fmt.Errorf("mongoexport failed: %w, output: %s", err, string(output))
	}

	// æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
	if _, err := os.Stat(outputPath); err != nil {
		return fmt.Errorf("mongoexport output file not created: %w", err)
	}

	// ç»Ÿè®¡å¯¼å‡ºçš„è®°å½•æ•°å’Œæ–‡ä»¶å¤§å°
	recordCount, fileSize, err := e.countRecordsInFile(outputPath)
	if err != nil {
		logrus.Warnf("[BackupExecutor] Failed to count records in %s: %v", outputPath, err)
		// å›é€€åˆ°åªæ˜¾ç¤ºæ–‡ä»¶å¤§å°
		if stat, err := os.Stat(outputPath); err == nil {
			logrus.Infof("[BackupExecutor] âœ… Mongoexport completed: %.2f MB", float64(stat.Size())/1024/1024)
		}
	} else {
		logrus.Infof("[BackupExecutor] âœ… Mongoexport completed: %d records, %.2f MB", recordCount, fileSize)
	}

	return nil
}

// countRecordsInFile ç»Ÿè®¡JSONæ–‡ä»¶ä¸­çš„è®°å½•æ•°é‡
func (e *BackupExecutor) countRecordsInFile(filePath string) (int, float64, error) {
	stat, err := os.Stat(filePath)
	if err != nil {
		return 0, 0, err
	}

	fileSize := float64(stat.Size()) / 1024 / 1024 // MB

	file, err := os.Open(filePath)
	if err != nil {
		return 0, fileSize, err
	}
	defer file.Close()

	scanner := bufio.NewScanner(file)
	count := 0

	for scanner.Scan() {
		line := strings.TrimSpace(scanner.Text())
		// è·³è¿‡ç©ºè¡Œå’ŒJSONæ•°ç»„æ ‡è®°
		if line != "" && line != "[" && line != "]" && line != "," {
			// ç®€å•æ£€æŸ¥æ˜¯å¦æ˜¯JSONå¯¹è±¡ï¼ˆä»¥{å¼€å¤´ï¼‰
			if strings.HasPrefix(line, "{") || (strings.HasPrefix(line, ",") && strings.Contains(line, "{")) {
				count++
			}
		}
	}

	if err := scanner.Err(); err != nil {
		return 0, fileSize, err
	}

	return count, fileSize, nil
}

// convertTimeRangeQuery Convert dynamic time range query to concrete MongoDB query
func (e *BackupExecutor) convertTimeRangeQuery(query map[string]interface{}) map[string]interface{} {
	result := make(map[string]interface{})

	for key, value := range query {
		if timeQuery, ok := value.(map[string]interface{}); ok {
			if timeType, exists := timeQuery["type"]; exists && timeType == "daily" {
				// Parse offset values
				startOffset := -1
				endOffset := 0

				if so, ok := timeQuery["startOffset"]; ok {
					if offset, ok := so.(float64); ok {
						startOffset = int(offset)
					}
				}
				if eo, ok := timeQuery["endOffset"]; ok {
					if offset, ok := eo.(float64); ok {
						endOffset = int(offset)
					}
				}

				// Calculate JST time range and convert to UTC for database query
				now := time.Now()
				jst := time.FixedZone("JST", 9*3600)

				// Get current JST time and truncate to start of day
				nowJST := now.In(jst)

				// Calculate start and end days in JST
				startDayJST := time.Date(nowJST.Year(), nowJST.Month(), nowJST.Day()+startOffset, 0, 0, 0, 0, jst)
				endDayJST := time.Date(nowJST.Year(), nowJST.Month(), nowJST.Day()+endOffset, 0, 0, 0, 0, jst)

				// Convert JST times to UTC
				startUTC := startDayJST.UTC()
				endUTC := endDayJST.UTC()

				logrus.Infof("[BackupExecutor] Time calculation: now=%s, startOffset=%d, endOffset=%d",
					nowJST.Format("2006-01-02 15:04:05 JST"), startOffset, endOffset)
				logrus.Infof("[BackupExecutor] JST range: %s to %s",
					startDayJST.Format("2006-01-02 15:04:05 JST"), endDayJST.Format("2006-01-02 15:04:05 JST"))
				logrus.Infof("[BackupExecutor] UTC range: %s to %s",
					startUTC.Format("2006-01-02T15:04:05.000Z"), endUTC.Format("2006-01-02T15:04:05.000Z"))

				// Create MongoDB date range query
				mongoQuery := map[string]interface{}{
					"$gte": map[string]interface{}{
						"$date": startUTC.Format("2006-01-02T15:04:05.000Z"),
					},
					"$lt": map[string]interface{}{
						"$date": endUTC.Format("2006-01-02T15:04:05.000Z"),
					},
				}

				result[key] = mongoQuery
				logrus.Infof("[BackupExecutor] Converted time range query for field %s: %s to %s",
					key, startUTC.Format("2006-01-02T15:04:05.000Z"), endUTC.Format("2006-01-02T15:04:05.000Z"))
			} else {
				// Keep non-time queries as is
				result[key] = value
			}
		} else {
			// Keep non-object values as is
			result[key] = value
		}
	}

	return result
}

// cleanQueryStringValues Clean string values in query condition to remove extra escaping
func cleanQueryStringValues(queryObj map[string]interface{}) map[string]interface{} {
	cleaned := make(map[string]interface{})

	for key, value := range queryObj {
		switch v := value.(type) {
		case string:
			// Remove surrounding quotes if they exist (handle over-escaping)
			cleanValue := v
			// Remove extra double quotes from the beginning and end
			if strings.HasPrefix(cleanValue, `"`) && strings.HasSuffix(cleanValue, `"`) {
				cleanValue = strings.TrimPrefix(cleanValue, `"`)
				cleanValue = strings.TrimSuffix(cleanValue, `"`)
			}
			// Remove extra single quotes from the beginning and end
			if strings.HasPrefix(cleanValue, `'`) && strings.HasSuffix(cleanValue, `'`) {
				cleanValue = strings.TrimPrefix(cleanValue, `'`)
				cleanValue = strings.TrimSuffix(cleanValue, `'`)
			}
			cleaned[key] = cleanValue
		case map[string]interface{}:
			// Recursively clean nested objects
			cleaned[key] = cleanQueryStringValues(v)
		default:
			// Keep other types as is
			cleaned[key] = value
		}
	}

	return cleaned
}

#!/bin/bash

# External Command-Based Backup Script
# é¿å…Goå†…å­˜ç®¡ç†é—®é¢˜ï¼Œä½¿ç”¨å¤–éƒ¨å‘½ä»¤å¤„ç†å¤§æ–‡ä»¶æ“ä½œ

set -e

# é…ç½®å‚æ•°
MONGO_URI="$1"
DATABASE="$2" 
COLLECTION="$3"
OUTPUT_DIR="$4"
GCS_BUCKET="$5"
DATE_STR="${6:-$(date +%Y-%m-%d)}"

# éªŒè¯å‚æ•°
if [ $# -lt 5 ]; then
    echo "Usage: $0 <mongo_uri> <database> <collection> <output_dir> <gcs_bucket> [date_str]"
    echo "Example: $0 'mongodb://user:pass@host:27017' mydb mycoll /tmp gs://my-bucket"
    exit 1
fi

# æ—¥å¿—å‡½æ•°
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
}

# å†…å­˜ç›‘æ§å‡½æ•°
monitor_memory() {
    local phase="$1"
    local mem_total=$(grep MemTotal /proc/meminfo | awk '{print int($2/1024)}')
    local mem_free=$(grep MemFree /proc/meminfo | awk '{print int($2/1024)}')
    local mem_avail=$(grep MemAvailable /proc/meminfo | awk '{print int($2/1024)}')
    log "ğŸ“Š Memory [$phase]: Total=${mem_total}MB, Free=${mem_free}MB, Available=${mem_avail}MB"
}

# é”™è¯¯å¤„ç†å‡½æ•°
cleanup_on_error() {
    local exit_code=$?
    log "âŒ Error occurred (exit code: $exit_code), cleaning up..."
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    [ -f "$JSON_FILE" ] && rm -f "$JSON_FILE"
    [ -f "$ZIP_FILE" ] && rm -f "$ZIP_FILE"
    
    # æ€æ­»å¯èƒ½è¿˜åœ¨è¿è¡Œçš„åå°è¿›ç¨‹
    pkill -f "mongoexport.*$COLLECTION" 2>/dev/null || true
    pkill -f "zip.*$COLLECTION" 2>/dev/null || true
    pkill -f "gsutil.*$COLLECTION" 2>/dev/null || true
    
    exit $exit_code
}

trap cleanup_on_error ERR INT TERM

# æ–‡ä»¶è·¯å¾„è®¾ç½®
JSON_FILE="${OUTPUT_DIR}/${COLLECTION}_${DATE_STR}.json"
ZIP_FILE="${OUTPUT_DIR}/${COLLECTION}_${DATE_STR}.zip"
GCS_PATH="gs://${GCS_BUCKET}/${COLLECTION}_${DATE_STR}.zip"

log "ğŸš€ Starting external command-based backup"
log "ğŸ“‹ Config: DB=$DATABASE, Collection=$COLLECTION, Output=$OUTPUT_DIR"

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
mkdir -p "$OUTPUT_DIR"

monitor_memory "START"

# Step 1: MongoDB Export with streaming to avoid memory buildup
log "ğŸ“¤ Step 1: MongoDB Export (streaming mode)"

# è·å–æ–‡æ¡£æ•°é‡
TOTAL_DOCS=$(mongo "$MONGO_URI" --quiet --eval "db.getSiblingDB('$DATABASE').$COLLECTION.countDocuments({})" 2>/dev/null || echo "0")
log "ğŸ“Š Total documents: $TOTAL_DOCS"

# ç›´æ¥ä¸€æ¬¡æ€§å¯¼å‡º - mongoexportä¼šè‡ªå·±ç®¡ç†å†…å­˜
log "ğŸ“„ Direct mongoexport (single export)"
mongoexport \
    --uri="$MONGO_URI" \
    --db="$DATABASE" \
    --collection="$COLLECTION" \
    --out="$JSON_FILE" \
    --quiet

# æ£€æŸ¥å¯¼å‡ºæ–‡ä»¶
if [ ! -f "$JSON_FILE" ]; then
    log "âŒ Export failed: JSON file not created"
    exit 1
fi

EXPORT_SIZE=$(stat -f%z "$JSON_FILE" 2>/dev/null || stat -c%s "$JSON_FILE" 2>/dev/null || echo "0")
EXPORT_SIZE_MB=$((EXPORT_SIZE / 1024 / 1024))
log "âœ… Export completed: ${EXPORT_SIZE_MB}MB"

monitor_memory "EXPORT_COMPLETE"

# Step 2: ZIP Compression (external zip process)
log "ğŸ—œï¸ Step 2: ZIP Compression (external process)"

# ä½¿ç”¨ç³»ç»Ÿzipå‘½ä»¤ï¼Œé¿å…Goå†…å­˜ç®¡ç†
cd "$OUTPUT_DIR"
zip -q "$ZIP_FILE" "$(basename "$JSON_FILE")"

# æ£€æŸ¥å‹ç¼©æ–‡ä»¶
if [ ! -f "$ZIP_FILE" ]; then
    log "âŒ Compression failed: ZIP file not created"
    exit 1
fi

ZIP_SIZE=$(stat -f%z "$ZIP_FILE" 2>/dev/null || stat -c%s "$ZIP_FILE" 2>/dev/null || echo "0")
ZIP_SIZE_MB=$((ZIP_SIZE / 1024 / 1024))
COMPRESSION_RATIO=$(( (EXPORT_SIZE - ZIP_SIZE) * 100 / EXPORT_SIZE ))

log "âœ… Compression completed: ${ZIP_SIZE_MB}MB (${COMPRESSION_RATIO}% reduction)"

monitor_memory "COMPRESSION_COMPLETE"

# Step 3: GCS Upload (external gsutil process)
log "â˜ï¸ Step 3: GCS Upload (external process)"

# ä½¿ç”¨gsutilçš„æµå¼ä¸Šä¼ ï¼Œé¿å…å†…å­˜ç¼“å­˜æ•´ä¸ªæ–‡ä»¶
gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp "$ZIP_FILE" "$GCS_PATH"

# éªŒè¯ä¸Šä¼ 
if gsutil ls "$GCS_PATH" >/dev/null 2>&1; then
    log "âœ… Upload completed: $GCS_PATH"
else
    log "âŒ Upload failed: Could not verify file in GCS"
    exit 1
fi

monitor_memory "UPLOAD_COMPLETE"

# Step 4: Cleanup
log "ğŸ§¹ Step 4: Cleanup"

# åˆ é™¤æœ¬åœ°ä¸´æ—¶æ–‡ä»¶
rm -f "$JSON_FILE"
rm -f "$ZIP_FILE"

log "ğŸ‰ External backup workflow completed successfully!"
log "ğŸ“Š Final stats: Collection=$COLLECTION, Documents=$TOTAL_DOCS, Compressed=${ZIP_SIZE_MB}MB"

monitor_memory "WORKFLOW_COMPLETE"